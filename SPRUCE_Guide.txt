SPRUCE Pipeline Guide

Before beginning, here are a few questions for your consideration:
1) What environmental and other spatial variables are likely to affect population structure, and are these available to download?
2) What is the geographic range you are interested in? Do you have reasonable coverage (genetic samples) over this range?
3) What time frame are you interested in? This will affect how you filter the IBD segments.

Note that Parts 1 and 2 can be done in either order or in parallel.

Part 1: Generate migration surfaces using MAPS (Software and information available here: https://github.com/halasadi/MAPS)

1.A: Prepare genomic data ("1A_Merge_Filter.sh" provided as an example)
Identify the genomic data you would like to include. Merge and filter, and split by chromosome. Data should be in plink format (bed/fam/bim).

1.B Perform haplotype phasing, exclude close relatives, and call identical by descent tracts ("1B_Phase_and_CallIBD.sh")
Alternative option using Beagle: https://github.com/halasadi/ibd_data_pipeline

1.C. Convert start and end positions for each IBD tract from position to centimorgans ("1C_Interpolate_Position_to_cM.R", adapted from https://github.com/halasadi/ibd_data_pipeline/blob/master/interpolate.R))

1.D Concatanate the ibd segments from each chromosom into one file (e.g. command below)

cat $IBD_OUT/Pagani_Scheinfeldt_ibd_chr*_merged_cM.ibd > $IBD_OUT/Pagani_Scheinfeldt_ibd_chr_all_merged_cM.ibd

1.E Convert the output of hapibd (.ibd file) to .sims format for MAPS ("1E_IBDfile_to_sims.sh", adapted from https://github.com/halasadi/MAPS/tree/master/convert_beagle)
You will need to edit script according to your selected IBD tract sizes (which correspond to different time intervals) and run separately for each IBD tract size

1.F Create and add accessory files (.outer, params-test-lower_upper.ini) to your MAPS working directory, and then run the MAPS software
You will need to run the software separately for each IBD tract size (i.e. time period)

1.G Run plotmaps (See "https://github.com/halasadi/plotmaps")
You may find a conda environment helpful to install plotmaps

R
library(plotmaps)
plot_maps(add.pts = TRUE, add.graph = TRUE, add.countries = FALSE,
	  longlat = FALSE, mcmcpath = "/Pagani_Scheinfeldt/2_6/2_6-MAPS-test-sim",
	  outpath = "/Pagani_Scheinfeldt/2_6/plotmaps", width = 10, height = 6)

plot_maps(add.pts = TRUE, add.graph = TRUE, add.countries = FALSE,
	  longlat = FALSE, mcmcpath = "/Pagani_Scheinfeldt/6_Inf/6_Inf-MAPS-test-sim",
	  outpath = "/Pagani_Scheinfeldt/6_Inf/plotmaps", width = 10, height = 6)


1.E Extract migration rate values from the MAPS output files 

A note on MAPS output files: most people do not interface directly with the output files from MAPS, so they are not well-described in the documentation. My steps to process these files is based on observation and brief correspondence with the MAPS autors.

First, to find migration rates take the values in the "mRates.txt" output file (m) and convert them to 10^m. 
Second, take the mean for each column (each column is a deme, and each row represents a posterior estimate of migration rate from the MCMC chain).
Third, the order of columns in "mRates.txt" corresponds to the output file called "demes.txt", which indicates the geographic coordinate of each deme.
Finally, cince migration rates were unbalanced among populations and skewed toward low values, we used a log transformation before including them as the response variable in the random forest model.

Part 2: 

2.A Prepare spatial variables ("GenerateSpatialVariables")
See also: https://github.com/evlynpless/MOSQLAND
In general: download spatial variables, crop to region of interest, and set the no data value to something sensible

2.B Extract environmental values from the coordinates corresponding to the demes in Step 1.E ("2_GenerateSpatialVariables/PrepareRasters_ExtractValues")

Part 3: Random forest regression ("3_SPRUCE_RandomForest.R")
Find associations between the environmental/spatial variables and the migration rates over space. Use cross-validation to validate.
User will need to run the script separately for each time period and should perform some tuning of the model.

